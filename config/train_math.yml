# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node:8 train.py config/train_gpt2.py

wandb_log: True
wandb_project: "nanoGPT-playground"

batch_size: 256
micro_batch_size: 256

gpt:
  block_size: 96
  n_layer: 6
  n_embd: 512
  n_head: 8
  vocab_size: 29
  dropout: 0.01
  bias: True

dataset: math-next

# this makes total number of tokens be 300B
max_iters: 100000
lr_decay_iters: 100000

# eval stuff
eval_interval: 200
eval_iters: 200
log_interval: 10

# weight decay
weight_decay: 1e-1
