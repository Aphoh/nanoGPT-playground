# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such
out_dir: "out-gpt2-mini-block"
eval_interval: 1000 # keep frequent because we'll overfit
eval_batches: 1
log_interval: 10 # don't print too too often
weight_decay: 1e-1

wandb_log: True # override via command line if you like
wandb_project: "gpt2-mini-block"
dataset: "openwebtext"
batch_size: 480
micro_batch_size: 12

# baby GPT model :)
gpt:
  block_size: 128
  n_layer: 3
  n_head: 4
  n_embd: 128
  dropout: 0.0
  # block linear stuff
  block_linear: True
  bl_b: 8

learning_rate: 6e-4
max_iters: 100000
lr_decay_iters: 100000 # make equal to max_iters usually
min_lr: -1.0 # compute this automatically learning_rate / 10 usually

warmup_iters: 100 # not super necessary potentially
